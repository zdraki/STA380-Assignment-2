---
title: 'STA 380, Part 2: Exercises 2'
output: pdf_document
---
```{r,include=FALSE}
library(knitr)
library(mosaic)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

opts_chunk$set(echo=FALSE,
               cache=TRUE, autodep=TRUE, cache.comments=FALSE,
               message=FALSE, warning=FALSE,results="hide")
```


#Flights at ABIA

Your task is to create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. You can annotate the figure and briefly describe it, but strive to make it as stand-alone as possible. It shouldn't need many, many paragraphs to convey its meaning. Rather, the figure should speak for itself as far as possible.

##Flights in and out of Austin in 2008!
```{r}
ABIA <- read.csv("~/GitHub/greendata/data/ABIA.csv")
summary(ABIA)
attach(ABIA)

#plot(UniqueCarrier)
library(dichromat)
library(ggplot2)
library(gplots)
```

```{r}
#In the following example, we split the complete dataset into individual 
#planes and then summarise each plane by counting the number of flights 
#(count = n()) and computing the average distance (dist = mean(Distance, na.rm = TRUE))
#and arrival delay (delay = mean(ArrDelay, na.rm = TRUE)). We then use ggplot2 to 
#display the output.

by_TailNum <- group_by(ABIA, TailNum)
delay <- summarise(by_TailNum,
                   count = n(),
                   dist = mean(Distance, na.rm = TRUE),
                   delay = mean(ArrDelay, na.rm = TRUE))
delay <- filter(delay, count > 20, dist < 2000)

# Interestingly, the average delay is  slightly related to the
# average distance flown by a plane.
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area()
```

We can see that the planes that fly from 550 to 770 miles are more prone to delays.This can be explained,since the median distance for a plane is 775 miles and there are less flights involving shorter/longer routes. Overall however the average delay is slightly related to the average distance flown by a plane.

In case some of us would like to see the arrival delays per carrier can see the following plots:

```{r}

ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = UniqueCarrier, y = ArrDelay))

ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = Month , y = DepDelay,color = UniqueCarrier))


```


```{r}
######
austin <- filter(ABIA, Origin == "AUS" | Dest == "AUS")
austin_delays2 <- austin %>%
  group_by(UniqueCarrier) %>%
  summarize(
    Delays = sum(DepDelay, na.rm = TRUE),
    Total = n(),
    Percent = round((Delays / Total) * 100,1)
  ) %>%
  arrange(desc(Percent)) 

# and a basic bar chart of the percentages

library(ggplot2)
ggplot(data = austin_delays2, aes(x=UniqueCarrier, y=Percent))  + geom_bar(stat="identity")

```
We can see that the "safest" option delay-wise is to fly with US airlines since they have the smallest delay percentage.\newpage


And if we wish to plan ahead our trip based on the delays per month in 2008 under the assumption that there is a pattern, we can consult the graphs below.

```{r}
ggplot(data = ABIA) + 
  geom_smooth(mapping = aes(x = Month, y = ArrDelay))

ggplot(data = ABIA) + 
  geom_smooth(mapping = aes(x = Month, y = DepDelay))
```

Based on these I would choose to fly sometime in October!
We can see that during the Christmas holidays and the summer period where the flight "load" is increased, the most delays occur.

#Author attribution

Revisit the Reuters C50 corpus that we explored in class. Your task is to build two separate models (using any combination of tools you see fit) for predicting the author of an article on the basis of that article's textual content. Describe clearly what models you are using, how you constructed features, and so forth. (Yes, this is a supervised learning task, but it potentially draws on a lot of what you know about unsupervised learning!)

In the C50train directory, you have ~50 articles from each of 50 different authors (one author per directory). Use this training data (and this data alone) to build the two models. Then apply your model to the articles by the same authors in the C50test directory, which is about the same size as the training set. How well do your models do at predicting the author identities in this out-of-sample setting? Are there any sets of authors whose articles seem difficult to distinguish from one another? Which model do you prefer?

First we read half of our data which consist of the training set.
The we modify the R.script ,that we studied in class, with a for loop so that it then contains all 50 authors.
Afterwards we strip our corpus of empty spaces and numbers, we make everything lowercase, we remove the stopwords  and we print here the term document matrix:

```{r}
library(tm)
library(foreach)

readerPlain = function(fname){
  readPlain(elem = list(content = readLines(fname)), 
            id =  fname, language = 'en') }

#/Users/Zenodrakos/R workspace/Predictive Modelling/Scott/HW2/ReutersC50/C50train
author_dirs = Sys.glob("/Users/Zenodrakos/R workspace/Predictive Modelling/Scott/HW2/ReutersC50/C50train/*")
#### Have to fix it so it contains all authors#######
#author_dirs = author_dirs[1:2]
file_list = NULL
labels = NULL

for(author in author_dirs) 
{
  author_name = substring(author, first = 82)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

# Creating training corpus
my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = file_list

# Preprocessing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))
# Creating DTM
DTM = DocumentTermMatrix(my_corpus)

```

```{r,results="show"}

DTM # some basic summary statistics


```

```{r}
## You can inspect its entries...
inspect(DTM[1:10,1:20])
DTM = removeSparseTerms(DTM, 0.975)
DTM
# Now a dense matrix
X = as.matrix(DTM)
smooth_count = 1/nrow(X)

```

```{r}

# Naive Bayes
#AP_train = X[1:45,]
#AC_train = X[51:95,]

# Naive Bayes: the training sets for all the authors
w = list()
smooth_count = 1/nrow(X)
j = 1
for (i in seq(1,length(file_list),50) )
{
  w[[j]] = colSums(X[i:(i+49),] + smooth_count)/sum(colSums(X[i:(i+49),] 
                                                            + smooth_count))
  j = j + 1
}


### TEST ###

readerPlain = function(fname){
  readPlain(elem = list(content = readLines(fname)), 
            id = fname, language = 'en') }
## Rolling two directories together into a single corpus
author_dirs_test = Sys.glob("/Users/Zenodrakos/R workspace/Predictive Modelling/Scott/HW2/ReutersC50/C50test/*")
#author_dirs = author_dirs[1:2]
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
  author_name = substring(author, first = 81)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

my_corpus_test = Corpus(VectorSource(all_docs_test))
names(my_corpus_test) = labels_test

# Preprocessing
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))

DTM_test = DocumentTermMatrix(my_corpus_test)
```
Then we will use the Na√Øve Bayes algorithm to the training set for all the authors while we put the 2 directories ensemble into a single corpus (we load the test data). We repeat again the processing procedure we did  for the training set before and we prind once more the term document matrix:

```{r,results="show"}
DTM_test # some basic summary statistics

```
We modify to keep only those words that we used in the training set and we print the DTM:
```{r}
# Keeping only those words which we used in the training set.
common_words = colnames(DTM_test)[colnames(DTM_test) %in% colnames(DTM)]
DTM_test <- DTM_test[, common_words]
```

```{r,results="show"}
DTM_test
```
Next step is to calculate the log probabilities (since the probabilities aone are very small and the computer cannot make comparisons of very small things),
```{r}
# Taking test documents and Comparing log probabilities 
X_test = as.matrix(DTM_test)
# Creating empty matrix to calculate log-probabilities
Y_test = matrix(, nrow = 2500, ncol = 50)
K = list()
j = 1
for (i in 1:2500)
{
  for (j in 1:50)
  {
    Y_test[i,j] = sum(X_test[i,]*log(w[[j]]))
  }
}
```
and find the document which corresponds to the maximum log-probability which in reality is the author.
```{r}
# Finding the document which corresponds to maximum log-probability (Hence the author)
# This can be done in a more readable way using for loop
library(dplyr)
author_predictions <- as.vector(t(as.data.frame(t(Y_test)) %>%
                                    summarise_each(funs(which.max(.) ) ) ) )
# Since authors are arranged so well with one author for every fifty files 
author_actual <- as.vector(rep(1:50,each=50))

library(caret)
library(e1071) # Weird. ConfusionMatrix asked for this library
confMatrix <- confusionMatrix(author_predictions,author_actual)
```
Finaly, our predictions can be summed into a (very big) confusion matrix and we see our predictions accuracy:
```{r,results="show"}
confMatrix
confMatrix$overall["Accuracy"]
```
 Summing up, the Naive Bayes model has an accuracy of approximately $60\%$ at predicting the author identities out of sample. Also by looking the sensitivity at every class we can say that the authors whose articles seem difficult to distinguish from one another are for example author 4 with a sensisitivity of $0.2$, author 8 with a sensitivity of $0.14$ and author 44 with a sensitivity of $0.26$.
 
#Practice with association rule mining

Use the data on grocery purchases in groceries.txt and find some interesting association rules for these shopping baskets. The data file is a list of baskets: one row per basket, with multiple items per row separated by commas -- you'll have to cobble together a few utilities for processing this into the format expected by the "arules" package. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries in an interesting and concise way.

We begin with a set of 5668 rules.
```{r}
library("arulesViz")
library(arules)
library(dichromat)
data("Groceries")
summary(Groceries)

rules <- apriori(Groceries, parameter=list(support=0.001, confidence=0.5))
rules
inspect(head(sort(rules, by ="lift"),3)) 
```
We can see that the 3 most popular rules acording to lift are:

53 {Instant food products,soda} => {hamburger meat}\
37 {soda,popcorn} => {salty snack} \
444 {flour,baking powder} => {sugar}\

Next we can see these very two informative scatterplots to visualise the different set of rules. It can also be helpful to pick the confidence,support and lift.
```{r,results="show"}
plot(rules, measure = "support", shading = "lift", data = Groceries)
plot(rules, measure=c("support", "lift"), shading="confidence")
```

By seting the confidence to be 0.8 or greater we see that we are left with a set of 371 rules.
```{r}
subrules <- rules[quality(rules)$confidence > 0.8]
subrules
```
The confidence of a rule is the likelihood that it is true 
for a new transaction that contains the items on the LHS of 
the rule. (I.e. it is the probability that the transaction 
also contains the item(s) on the RHS.) Formally:
  
The lift of a rule is the ratio of the support of the items 
on the LHS of the rule co-occuring with items on the RHS 
divided by probability that the LHS and RHS co-occur if the 
two are independent.  \


If lift is greater than 1, it suggests that the precense of 
the items on the LHS has increased the probability that the 
items on the right hand side will occur on this transaction. 
If the lift is below 1, it suggests that the presence of the 
items on the LHS make the probability that the items on the 
RHS will be part of the transaction lower. If the lift is 1, 
it suggests that the presence of items on the LHS and RHS 
really are independent: knowing that the items on the LHS are 
present makes no difference to the probability that items will
occur on the RHS.

Another set of plots that gives us information about the set of rules at hand can be seen below:


```{r,results="hide"}
plot(subrules, method="matrix", measure="lift")

plot(subrules, method="matrix", measure="lift", control=list(reorder=TRUE))
```

```{r}
rules <- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8)) # I removed maxlen here
rules <- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8,maxlen=4))
# Show the top Y rules, but only 2 digits
options(digits=2)
#inspect(rules)
inspect(rules[1:20])



```

Here is a balloon plot with our grouped relationships:
```{r,results="show"}
plot(rules, method="grouped")
```
And another one sorted by lift:
```{r,results="show"}
subrules2 <- head(sort(rules, by="lift"), 10)
plot(subrules2, method="graph")
```


Given that, I will use a support of 0.001 and confidence of 0.8.
The summary of our rules then is:
```{r}
summary(rules)
```

Next we can see what is most probable to hapen if we list the rules by confidence.
```{r}
#Show and plot the rules by confidence. What is most probable to happen
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:20])
```

1 {rice,sugar}                                => {whole milk}\ 
2 {canned fish,hygiene articles}              => {whole milk} \  
3 {root vegetables,butter,rice}               => {whole milk}\   
4 {root vegetables,whipped/sour cream,flour}  => {whole milk}\
5 {butter,soft cheese,domestic eggs}          => {whole milk} \

```{r}
subset.matrix <- is.subset(rules, rules)
subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
redundant <- colSums(subset.matrix, na.rm=T) >= 1
rules.pruned <- rules[!redundant]
rules<-rules.pruned
```

Also,
```{r,results="show"}
plot(rules) # rules with high lift have typically a relatively low support. Do not say it
plot(rules, measure=c("support", "lift"), shading="confidence")

head(quality(rules))   
```

Now from our first frequency plot, we will emphasize on the first 5 selling items:

 What are the customersmore likely to buy before buying whole milk?
 

```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.8), 
               appearance = list(default="lhs",rhs="whole milk"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
plot(rules,method="graph",shading=NA)
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are the customersmore likely to buy before buying other veggies?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.8), 
               appearance = list(default="lhs",rhs="other vegetables"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are the customersmore likely to buy before buying rolls/buns?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.62), #the greatest confidence is 0.62.There is no result for greatest certainty
               appearance = list(default="lhs",rhs="rolls/buns"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```


What are the customersmore likely to buy before buying soda?
```{r,results="show"}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.65), 
               appearance = list(default="lhs",rhs="soda"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r}
plot(rules,method="graph",shading=NA)
```


What are the customersmore likely to buy before buying yogurt?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.65), 
               appearance = list(default="lhs",rhs="soda"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are customers likely to buy if they purchase whole milk?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
               appearance = list(default="rhs",lhs="whole milk"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are customers likely to buy if they purchase other vegetables?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
               appearance = list(default="rhs",lhs="other vegetables"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are customers likely to buy if they purchase rolls/buns?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
               appearance = list(default="rhs",lhs="rolls/buns"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are customers likely to buy if they purchase soda?

```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
               appearance = list(default="rhs",lhs="soda"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```

What are customers likely to buy if they purchase yogurt?
```{r}
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
               appearance = list(default="rhs",lhs="yogurt"),
               control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
```

```{r,results="show"}
plot(rules,method="graph",shading=NA)
```


